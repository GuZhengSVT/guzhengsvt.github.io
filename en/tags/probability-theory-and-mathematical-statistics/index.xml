<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Probability Theory and Mathematical Statistics on 孤筝の温暖小家</title><link>https://guzhengsvt.cn/en/tags/probability-theory-and-mathematical-statistics/</link><description>Recent content from 孤筝の温暖小家</description><generator>Hugo</generator><language>en</language><managingEditor>lvbowen040427@163.com (孤筝)</managingEditor><webMaster>lvbowen040427@163.com (孤筝)</webMaster><copyright>All website licensed under CC BY 4.0</copyright><lastBuildDate>Tue, 10 Sep 2024 01:14:05 +0800</lastBuildDate><atom:link href="https://guzhengsvt.cn/en/tags/probability-theory-and-mathematical-statistics/index.xml" rel="self" type="application/rss+xml"/><item><title>Probability Theory and Mathematical Statistics</title><link>https://guzhengsvt.cn/en/post/math/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/</link><pubDate>Tue, 10 Sep 2024 01:14:05 +0800</pubDate><author>lvbowen040427@163.com (孤筝)</author><guid>https://guzhengsvt.cn/en/post/math/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/</guid><description><![CDATA[<h1>Probability Theory and Mathematical Statistics</h1><p>作者: 孤筝 (lvbowen040427@163.com)</p>
          <h2 id="preface">
<a class="header-anchor" href="#preface"></a>
Preface
</h2><h3 id="first-edition-preface">
<a class="header-anchor" href="#first-edition-preface"></a>
First Edition Preface
</h3><p>[[2024-09-14]] Today, the makeup exam finally ended. I heard the original exam directly reused past papers. Over the past few days, I practiced three sets of &ldquo;XDU&rsquo;s original exam papers&rdquo; (from 2021 and two from 2023) found online. The morning session covered the 2021 paper, and in the afternoon, <strong>a quarter</strong> of the questions were identical word-for-word. I couldn&rsquo;t help but laugh.</p>
<p>Dai Hao once said he would try his best to assign the best teachers to the Qian Class. But now, it seems the School of Mathematics and Statistics has run out of competent faculty? Poor teaching could be excused as a lack of focus on pedagogy or innate teaching talent. However, reusing recent exam papers verbatim, riddled with errors, is downright laughable.</p>
<p>The self-made exam papers lack any originality, and the creators didn’t even bother to review them. This is an attitude problem.</p>
<p>It’s fine for your university to go easy on finals, but don’t keep recycling old material. You preach innovation to students yet settle for mediocrity yourselves. This is neither the attitude for academia nor for teaching.</p>
<p>Probability Theory ends here. Over the past two days, I repeatedly reviewed notes, solved problems, and corrected many errors, clarifying the knowledge structure of this course. Although the content remains relatively sparse, it should suffice as final review material. This edition will likely be the final version (probably).</p>
<p>During the Mid-Autumn Festival, I’ll continue organizing Electrodynamics and Digital Signal Processing.</p>
<h3 id="second-edition-preface">
<a class="header-anchor" href="#second-edition-preface"></a>
Second Edition Preface
</h3><blockquote>
<p>Nothing is final!!!<br>
—Qian Xuesen</p></blockquote>
<p>Added clarification on the left/right continuity of distribution functions. It seems this course is far from &ldquo;final&rdquo;&hellip;</p>
<hr>
<h2 id="event-operations-to-logical-operations">
<a class="header-anchor" href="#event-operations-to-logical-operations"></a>
Event Operations to Logical Operations
</h2><ul>
<li>$A \cup B = A + B$</li>
<li>$A \cap B = A \cdot B$</li>
<li>$A - B = A \bar{B}$<br>
Event $A$ occurs while $B$ does not, easily verified via Venn diagrams.<br>
Interpret $-B$ as $\cdot (-B)$, where $-B$ is $\bar{B}$.</li>
<li>If $A \subset B$, then $A \cup B = B$ and $A \cap B = A$.</li>
</ul>
<p>After converting event operations to logical operations, most rules align.<br>
Use logical function simplification from digital electronics to reduce complex event operations.<br>
<strong>Tip</strong>: Karnaugh maps.</p>
<hr>
<h2 id="four-major-probability-formulas">
<a class="header-anchor" href="#four-major-probability-formulas"></a>
Four Major Probability Formulas
</h2>$$
\begin{cases}
P(A+B) = P(A) + P(B) - P(AB) \\
P(A-B) = P(A) - P(AB) = P(A \bar{B}) \\
P(AB) = P(B) \cdot P(A|B) = P(A) \cdot P(B|A) \\
P(A|B) = \frac{P(AB)}{P(B)} \\
\end{cases}
$$<h3 id="corollaries">
<a class="header-anchor" href="#corollaries"></a>
Corollaries
</h3><p>For $P(A+B+C)$, treat $A+B$ as a single event and apply the addition formula twice:<br>
</p>
$$
P(A+B+C) = P(A) + P(B) + P(C) - P(AB) - P(AC) - P(BC) + P(ABC)
$$<p><br>
Larger unions follow recursively.</p>
<p>Complementary event: Probability of $A$ not occurring, obvious from Venn diagrams.<br>
</p>
$$
P(\bar{A}) = P(1 \cdot \bar{A}) = P(1 - A) = P(1) - P(1 \cdot A) = 1 - P(A)
$$<hr>
<h2 id="non-negativity-and-normalization">
<a class="header-anchor" href="#non-negativity-and-normalization"></a>
Non-Negativity and Normalization
</h2><ul>
<li><strong>Non-negativity</strong>: For any event $A$, $0 \le P(A) \le 1$.</li>
<li><strong>Normalization</strong>: For the sample space $\Omega$, $P(\Omega) = 1$.</li>
</ul>
<hr>
<h2 id="independence">
<a class="header-anchor" href="#independence"></a>
Independence
</h2>$$
\begin{cases}
P(AB) = P(A) \cdot P(B) \\
P(A|B) = P(A) \\
\end{cases}
$$<p><br>
Independence implies mutual independence.</p>
<hr>
<h2 id="classical-probability-model">
<a class="header-anchor" href="#classical-probability-model"></a>
Classical Probability Model
</h2><blockquote>
<p>All elementary events are equally likely.</p></blockquote>
<p><strong>E.g.</strong>, coin tosses, dice rolls&hellip;<br>
</p>
$$
P(A) = \frac{\text{Number of elementary events in } A}{\text{Total elementary events in } \Omega}
$$<p><br>
Classical conditional probability formula:<br>
</p>
$$
P(B|A) = \frac{P(AB)}{P(A)} = \frac{\text{Elementary events in both } A \text{ and } B}{\text{Elementary events in } A}
$$<hr>
<h2 id="bernoulli-trials-binomial-distribution">
<a class="header-anchor" href="#bernoulli-trials-binomial-distribution"></a>
Bernoulli Trials (Binomial Distribution)
</h2><blockquote>
<p>$n$ independent trials, each with outcomes $A$ or $\bar{A}$.</p></blockquote>
<p>$X \sim B(n, p)$<br>
</p>
$$
P_n(k) = C_n^k p^k (1-p)^{n-k}
$$<p><br>
where $p = P(A)$, $1-p = P(\bar{A})$.</p>
<hr>
<h2 id="geometric-probability-model">
<a class="header-anchor" href="#geometric-probability-model"></a>
Geometric Probability Model
</h2><p>The ratio of the <em>length/area/volume</em> occupied by an event to the total <em>length/area/volume</em> of $\Omega$.<br>
If the event&rsquo;s dimension is lower than $\Omega$&rsquo;s, its probability is always 0.<br>
<strong>Warning</strong>: A probability of 0 doesn’t mean the event is impossible.<br>
<strong>E.g.</strong>: Randomly selecting a point inside a circle—the probability of any specific point is 0, but it can still occur.</p>
<hr>
<h2 id="uniform-distribution">
<a class="header-anchor" href="#uniform-distribution"></a>
Uniform Distribution
</h2><p>$x \sim U(a, b)$<br>
Linear approximation in geometric distributions. Probability density:<br>
</p>
$$
f(x) =
\begin{cases}
0, & x \le a \\
\frac{1}{b-a}, & a < x \le b \\
0, & x > b \\
\end{cases}
$$<p><br>
Cumulative distribution function (CDF):<br>
</p>
$$
F(x) =
\begin{cases}
0, & x \le a \\
\frac{x - a}{b - a}, & a < x \le b \\
1, & x > b \\
\end{cases}
$$<hr>
<h2 id="exponential-distribution">
<a class="header-anchor" href="#exponential-distribution"></a>
Exponential Distribution
</h2><p>$x \sim E(\lambda)$</p>
<h3 id="probability-density">
<a class="header-anchor" href="#probability-density"></a>
Probability Density
</h3>$$
f(x) =
\begin{cases}
\lambda e^{-\lambda x}, & x > 0 \\
0, & x \le 0 \\
\end{cases}
$$<h3 id="cdf">
<a class="header-anchor" href="#cdf"></a>
CDF
</h3>$$
F(x) =
\begin{cases}
1 - e^{-\lambda x}, & x \ge 0 \\
0, & x < 0 \\
\end{cases}
$$<hr>
<h2 id="poisson-distribution">
<a class="header-anchor" href="#poisson-distribution"></a>
Poisson Distribution
</h2><p>$X \sim \pi(\lambda)$<br>
</p>
$$
P(X = k) = \frac{e^{-\lambda} \lambda^k}{k!}
$$<hr>
<h2 id="normal-distribution">
<a class="header-anchor" href="#normal-distribution"></a>
Normal Distribution
</h2><p>$x \sim N(\mu, \sigma^2)$</p>
<h3 id="probability-density-1">
<a class="header-anchor" href="#probability-density-1"></a>
Probability Density
</h3>$$
f(x) = \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(x - \mu)^2}{2 \sigma^2}}, \quad x \in \mathbb{R}, \sigma > 0
$$<h3 id="cdf-1">
<a class="header-anchor" href="#cdf-1"></a>
CDF
</h3>$$
F(x) = \int_{-\infty}^x f(t) \, dt
$$<p><br>
Clearly, $F(\mu) = \frac{1}{2}$, i.e., $P(x \le \mu) = P(x > \mu) = \frac{1}{2}$.</p>
<h3 id="standard-normal-distribution">
<a class="header-anchor" href="#standard-normal-distribution"></a>
Standard Normal Distribution
</h3><p>When $\mu = 0$, $\sigma = 1$, it becomes the standard normal distribution.<br>
</p>
$$
\varphi(x) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{x^2}{2}}
$$<p><br>
</p>
$$
\Phi(x) = \int_{-\infty}^x \varphi(t) \, dt
$$<h3 id="corollaries-1">
<a class="header-anchor" href="#corollaries-1"></a>
Corollaries
</h3>$$
\Phi(-x) = 1 - \Phi(x)
$$<p><br>
</p>
$$
F(x) = \Phi\left(\frac{x - \mu}{\sigma}\right)
$$<p><br>
Normalization:<br>
</p>
$$
X \sim N(\mu, \sigma^2), \quad Z = \frac{X - \mu}{\sigma} \sim N(0, 1)
$$<hr>
<h2 id="total-probability-formula">
<a class="header-anchor" href="#total-probability-formula"></a>
Total Probability Formula
</h2><h3 id="complete-event-group">
<a class="header-anchor" href="#complete-event-group"></a>
Complete Event Group
</h3>$$
\begin{cases}
B_1 \cup B_2 \cup \cdots \cup B_n = \Omega \\
B_i \cap B_j = \varnothing, \quad i \ne j, 1 \le i, j \le n \\
\end{cases}
$$<p><br>
Here, $B_1, B_2, \dots, B_n$ form a complete event group of $\Omega$.</p>
<h3 id="total-probability-formula-1">
<a class="header-anchor" href="#total-probability-formula-1"></a>
Total Probability Formula
</h3>$$
\begin{align}
P(A) &= P(AB_1 \cup AB_2 \cup \cdots \cup AB_n) \\
&= P(AB_1) + P(AB_2) + \cdots + P(AB_n) \\
&= P(B_1)P(A|B_1) + P(B_2)P(A|B_2) + \cdots + P(B_n)P(A|B_n) \\
\end{align}
$$<h3 id="bayes-formula">
<a class="header-anchor" href="#bayes-formula"></a>
Bayes&rsquo; Formula
</h3>$$
P(B_1|A) = \frac{P(AB_1)}{P(A)} = \frac{P(B_1)P(A|B_1)}{P(A)}
$$<hr>
<h2 id="one-dimensional-discrete-random-variables">
<a class="header-anchor" href="#one-dimensional-discrete-random-variables"></a>
One-Dimensional Discrete Random Variables
</h2><h3 id="probability-mass-function-pmf">
<a class="header-anchor" href="#probability-mass-function-pmf"></a>
Probability Mass Function (PMF)
</h3>$$
P(X = x_i) = p_i = \frac{\text{Count of } X = x_i}{\text{Total cases}}, \quad i = 1, 2, \dots
$$<h3 id="cdf-2">
<a class="header-anchor" href="#cdf-2"></a>
CDF
</h3>$$
F(x) = \sum_{x_i < x} p_i, \quad x \in \mathbb{R}
$$<hr>
<h2 id="one-dimensional-continuous-random-variables">
<a class="header-anchor" href="#one-dimensional-continuous-random-variables"></a>
One-Dimensional Continuous Random Variables
</h2><h3 id="probability-density-function-pdf">
<a class="header-anchor" href="#probability-density-function-pdf"></a>
Probability Density Function (PDF)
</h3>$$
f(x) = F'(x)
$$<h3 id="cdf-3">
<a class="header-anchor" href="#cdf-3"></a>
CDF
</h3>$$
F(x) = \int_{-\infty}^x f(t) \, dt
$$<h3 id="interval-probability">
<a class="header-anchor" href="#interval-probability"></a>
Interval Probability
</h3>$$
P(a < x \le b) = \int_a^b f(x) \, dx = F(b) - F(a)
$$<p><br>
Since $P(x = a) = 0$ for any $a \in \mathbb{R}$, interval endpoints can be inclusive/exclusive.</p>
<h3 id="normalization">
<a class="header-anchor" href="#normalization"></a>
Normalization
</h3>$$
F(\infty) = \int_{-\infty}^\infty f(x) \, dx = 1
$$<p><br>
</p>
$$
F(-\infty) = 0
$$<hr>
<h2 id="two-dimensional-discrete-random-variables">
<a class="header-anchor" href="#two-dimensional-discrete-random-variables"></a>
Two-Dimensional Discrete Random Variables
</h2><h3 id="joint-pmf">
<a class="header-anchor" href="#joint-pmf"></a>
Joint PMF
</h3><p>$P(X = x_i, Y = y_j)$<br>
Create a 2D table of possible $(X, Y)$ values and fill in probabilities.</p>
<h3 id="marginal-pmf">
<a class="header-anchor" href="#marginal-pmf"></a>
Marginal PMF
</h3><p>$P(X = x_i)$, $P(Y = y_j)$<br>
Sum rows/columns of the joint PMF to obtain $f_Y(x)$, $f_X(y)$.</p>
<h3 id="conditional-distribution">
<a class="header-anchor" href="#conditional-distribution"></a>
Conditional Distribution
</h3><p>$P(X = x_i | Y = y_j)$, $P(Y = y_i | X = x_j)$<br>
Divide each row/column of the joint PMF by its marginal PMF.<br>
Each row/column becomes <strong>proportional</strong>, summing to 1.</p>
<h3 id="independence-of-variables">
<a class="header-anchor" href="#independence-of-variables"></a>
Independence of Variables
</h3><p><strong>Note</strong>: Independence here refers to linear independence, not complete independence.<br>
Write the joint PMF as a matrix $\vec{A}$. If $\det \vec{A} = 0$, $X$ and $Y$ are independent.<br>
Alternatively: Rows/columns of the joint PMF are proportional.<br>
Or: If $P(X = x_i, Y = y_j) \ne P(X = x_i)P(Y = y_j)$, $X$ and $Y$ are dependent.</p>
<hr>
<h2 id="two-dimensional-continuous-random-variables">
<a class="header-anchor" href="#two-dimensional-continuous-random-variables"></a>
Two-Dimensional Continuous Random Variables
</h2><h3 id="joint-pdf">
<a class="header-anchor" href="#joint-pdf"></a>
Joint PDF
</h3>$$
f(x, y)
$$<h3 id="normalization-1">
<a class="header-anchor" href="#normalization-1"></a>
Normalization
</h3>$$
\int_{-\infty}^\infty \int_{-\infty}^\infty f(x, y) \, dx \, dy = 1
$$<h3 id="marginal-pdf">
<a class="header-anchor" href="#marginal-pdf"></a>
Marginal PDF
</h3>$$
f_X(x) = \int_{-\infty}^\infty f(x, y) \, dy
$$<p><br>
</p>
$$
f_Y(y) = \int_{-\infty}^\infty f(x, y) \, dx
$$<h3 id="conditional-pdf">
<a class="header-anchor" href="#conditional-pdf"></a>
Conditional PDF
</h3>$$
f_{Y|X}(y | x) = \frac{f(x, y)}{f_X(x)}
$$<h3 id="independence-1">
<a class="header-anchor" href="#independence-1"></a>
Independence
</h3>$$
f(x, y) = f_X(x) f_Y(y)
$$<p><br>
If satisfied, $X$ and $Y$ are independent.</p>
<h3 id="cdf-4">
<a class="header-anchor" href="#cdf-4"></a>
CDF
</h3><p>Let $Z = X - Y$,<br>
</p>
$$
\begin{align}
F_Z(z) &= P(Z < z) \\
&= P(X - Y < z) \\
&= P(X < Y + z) \\
&= \int_{-\infty}^y \int_{-\infty}^{y + z} f(x, y) \, dx \, dy \\
\end{align}
$$<p><br>
Thus, $F_Z(z) = \iint_D f(x, y) \, dx \, dy$. Differentiate $F_Z(z)$ to get $f_Z(z)$.<br>
<strong>Warning</strong>: $F_Z(z)$ must satisfy normalization.</p>
<hr>
<h2 id="expectation-and-variance">
<a class="header-anchor" href="#expectation-and-variance"></a>
Expectation and Variance
</h2><h3 id="key-relations">
<a class="header-anchor" href="#key-relations"></a>
Key Relations
</h3>$$
D(X) = E(X^2) - [E(X)]^2
$$<p><br>
</p>
$$
D(cX) = c^2 D(X)
$$<p><br>
</p>
$$
D(X + Y) = D(X) + D(Y) + 2 \text{Cov}(X, Y)
$$<p><br>
If $X$ and $Y$ are independent, $\text{Cov}(X, Y) = 0$.</p>
<h3 id="common-expectations-and-variances">
<a class="header-anchor" href="#common-expectations-and-variances"></a>
Common Expectations and Variances
</h3><h4 id="distribution">
<a class="header-anchor" href="#distribution"></a>
$(0,1)$ Distribution
</h4>$$
E(X) = p, \quad D(X) = p(1 - p)
$$<h4 id="binomial-distribution">
<a class="header-anchor" href="#binomial-distribution"></a>
$B(n, p)$ Binomial Distribution
</h4>$$
E(X) = np, \quad D(X) = np(1 - p)
$$<h4 id="uniform-distribution-1">
<a class="header-anchor" href="#uniform-distribution-1"></a>
$U(a, b)$ Uniform Distribution
</h4>$$
E(X) = \frac{a + b}{2}, \quad D(X) = \frac{(b - a)^2}{12}
$$<h4 id="exponential-distribution-1">
<a class="header-anchor" href="#exponential-distribution-1"></a>
$E(\lambda)$ Exponential Distribution
</h4>$$
E(X) = \frac{1}{\lambda}, \quad D(X) = \frac{1}{\lambda^2}
$$<h4 id="poisson-distribution-1">
<a class="header-anchor" href="#poisson-distribution-1"></a>
$P(\lambda)$ Poisson Distribution
</h4>$$
E(X) = \lambda, \quad D(X) = \lambda
$$<h4 id="normal-distribution-1">
<a class="header-anchor" href="#normal-distribution-1"></a>
$N(\mu, \sigma^2)$ Normal Distribution
</h4>$$
E(X) = \mu, \quad D(X) = \sigma^2
$$<hr>
<h2 id="covariance-and-correlation">
<a class="header-anchor" href="#covariance-and-correlation"></a>
Covariance and Correlation
</h2><h3 id="covariance">
<a class="header-anchor" href="#covariance"></a>
Covariance
</h3>$$
\text{Cov}(X, Y) = E(XY) - E(X)E(Y)
$$<p><br>
Clearly, if $X = Y$, $\text{Cov}(X, X) = D(X)$.<br>
</p>
$$
\text{Cov}(X + Y, Z) = \text{Cov}(X, Z) + \text{Cov}(Y, Z)
$$<p><br>
</p>
$$
\text{Cov}(X - Y, Z) = \text{Cov}(X, Z) - \text{Cov}(Y, Z)
$$<h3 id="correlation-coefficient">
<a class="header-anchor" href="#correlation-coefficient"></a>
Correlation Coefficient
</h3>$$
\rho_{XY} = \frac{\text{Cov}(X, Y)}{\sqrt{D(X) D(Y)}}
$$<p><br>
Higher $|\rho|$ indicates stronger correlation.</p>
<ul>
<li>If $Y = X$, $\rho = 1$ (perfect positive correlation).</li>
<li>If $Y = -X$, $\rho = -1$ (perfect negative correlation).<br>
Thus, $|\rho| \le 1$.</li>
<li>$\rho = 0$ implies no correlation.<br>
<strong>Warning</strong>: No correlation $\nRightarrow$ independence, but independence $\Rightarrow$ no correlation.</li>
</ul>
<hr>
<h2 id="chebyshevs-inequality-for-probability-estimation">
<a class="header-anchor" href="#chebyshevs-inequality-for-probability-estimation"></a>
Chebyshev&rsquo;s Inequality for Probability Estimation
</h2>$$
P(|X - E(X)| \ge \varepsilon) \le \frac{D(X)}{\varepsilon^2}
$$<hr>
<h2 id="central-limit-theorem">
<a class="header-anchor" href="#central-limit-theorem"></a>
Central Limit Theorem
</h2><p>For a large number of independent, identically distributed variables, their sum approximates a normal distribution.<br>
If $x_1, x_2, \dots, x_n$ are independent and identically distributed, then<br>
</p>
$$
\sum_{i=1}^n x_i \sim N\left(\sum_{i=1}^n E(x_i), \sum_{i=1}^n D(x_i)\right)
$$<hr>
<h2 id="three-major-distributions">
<a class="header-anchor" href="#three-major-distributions"></a>
Three Major Distributions
</h2><h3 id="chi-squared-distribution">
<a class="header-anchor" href="#chi-squared-distribution"></a>
$\chi^2$ (Chi-Squared) Distribution
</h3>$$
X = x_1^2 + x_2^2 + \cdots + x_n^2 \sim \chi^2(n), \quad x_i \sim N(0,1) \text{ and independent}
$$<p><br>
Upper $\alpha$ quantile: $\chi^2_\alpha(n)$.<br>
Density function is defined in the first quadrant.</p>
<h3 id="-distribution">
<a class="header-anchor" href="#-distribution"></a>
$t$-Distribution
</h3>$$
X = \frac{x_1}{\sqrt{x_2 / n}} \sim t(n), \quad x_1 \sim N(0,1), x_2 \sim \chi^2(n), \text{ and independent}
$$<p><br>
Upper $\alpha$ quantile: $t_\alpha(n)$.<br>
Density function resembles a normal distribution, symmetric about zero.</p>
<h3 id="-distribution-1">
<a class="header-anchor" href="#-distribution-1"></a>
$F$-Distribution
</h3>$$
X = \frac{x_1 / n_1}{x_2 / n_2} \sim F(n_1, n_2), \quad x_1 \sim \chi^2(n_1), x_2 \sim \chi^2(n_2), \text{ and independent}
$$<p><br>
Upper $\alpha$ quantile: $F_\alpha(n_1, n_2)$.<br>
Density function is defined in the first quadrant.</p>
<hr>
<h2 id="estimation-methods">
<a class="header-anchor" href="#estimation-methods"></a>
Estimation Methods
</h2><p>For simple random samples that are independent and identically distributed, estimate unknown parameters.</p>
<h3 id="method-of-moments">
<a class="header-anchor" href="#method-of-moments"></a>
Method of Moments
</h3><p>For large samples, approximate the sample mean as the population mean (moment matching).</p>
<ol>
<li>Derive $E(X)$ (first population moment) from the given PMF/PDF.</li>
<li>Compute the sample mean $\bar{X}$ (first sample moment).</li>
<li>Solve $E(X) = \bar{X}$ for $\theta_0$, yielding the estimate $\hat{\theta}$.</li>
</ol>
<h3 id="maximum-likelihood-estimation">
<a class="header-anchor" href="#maximum-likelihood-estimation"></a>
Maximum Likelihood Estimation
</h3><p>Estimate $\theta$ to maximize the probability of observing the sample.<br>
Likelihood function:<br>
</p>
$$
L(x_1, x_2, \dots, x_n; \theta) =
\begin{cases}
P(X = x_1) P(X = x_2) \cdots P(X = x_n), & \text{discrete} \\
f(x_1; \theta) f(x_2; \theta) \cdots f(x_n; \theta), & \text{continuous} \\
\end{cases}
$$<p><br>
Maximize $L$ by solving for its critical points. For computational ease, take the natural logarithm first:<br>
</p>
$$
(\ln L)' =
\begin{cases}
(\ln P_1 + \ln P_2 + \cdots + \ln P_n)', & \text{discrete} \\
[\ln f(x_1; \theta) + \ln f(x_2; \theta) + \cdots + \ln f(x_n; \theta)]', & \text{continuous} \\
\end{cases}
= 0
$$<p><br>
The solution $\theta_0$ is the estimate $\hat{\theta}$.</p>
<hr>
<h2 id="unbiasedness-and-efficiency">
<a class="header-anchor" href="#unbiasedness-and-efficiency"></a>
Unbiasedness and Efficiency
</h2><p>If $E(\hat{\theta}) = \theta$, $\hat{\theta}$ is an unbiased estimator.<br>
If $\</p>

        <hr><p>本文 2024-09-10 首发于 <a href='https://guzhengsvt.cn/'>孤筝の温暖小家</a>，最后修改于 2024-09-10</p><p>All website licensed under CC BY 4.0</p>]]></description><category>Math</category></item></channel></rss>